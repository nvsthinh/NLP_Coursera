# 2. Week 2: Sentiment Analysis with Naive Bayes

## 2.1. **Probability and Bayesâ€™ Rule**

TÆ°á»Ÿng tÆ°á»£ng ráº±ng báº¡n cÃ³ má»™t corpus of tweets cÃ³ thá»ƒ phÃ¢n loáº¡i giá»¯a positive vÃ  negative. Äá»ƒ tÃ­ch xÃ¡c suáº¥t cá»§a Positive lÃ  Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n nhÆ° á»Ÿ dÆ°á»›i Ä‘Ã¢y

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W210.png" width="500"/>
</p>

Tá»›i vá»›i vÃ­ dá»¥ tiáº¿p theo, vá»›i dÃ²ng tweets cÃ³ chá»©a â€œhappyâ€ trong cÃ¢u

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W211.png" width="500"/>
</p>

## 2.2. Bayeâ€™s Rule

Conditional probabilities giÃºp chÃºng ta giáº£m khÃ´ng giam tÃ¬m kiáº¿m máº«u. Vá»›i vÃ­ dá»¥ giá»‘ng nhÆ° trÃªn:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W220.png" width="500"/>
</p>

Vá»›i cÃ´ng thá»©c nhÆ° nÃ y, thÃ¬ viá»‡c tÃ¬m kiáº¿m $P("happy")$ vÃ  $P(Positive\cap"happy")$ dá»… dÃ ng hÆ¡n lÃ  tÃ­nh $P(Positive|"happy")$

Vá»›i cÃ´ng thá»©c cá»§a Conditional Probability. Vá»›i hai trÆ°á»ng há»£p $P(Positive|"happy")$ vÃ  $P("happy"|Positive)$. Sau Ä‘Ã³, chÃºng ta chá»‰ tÃ¬m kiáº¿m trong vÃ²ng trÃ²n mÃ u xanh á»Ÿ trÃªn. Tá»­ sá»‘ sáº½ lÃ  pháº§n mÃ u Ä‘á» vÃ  máº«u sá»‘ sáº½ lÃ  pháº§n mÃ u xanh. Äiá»u nÃ y dáº«n chÃºng ta Ä‘áº¿n káº¿t luáº­n sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W221.png" width="500"/>
</p>

Thay tá»­ sá»‘ vÃ o váº¿ pháº£i cá»§a phÆ°Æ¡ng trÃ¬nh Ä‘áº§u tiÃªn, chÃºng tasáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W222.png" width="500"/>
</p>

**â†’ Bayes Rule**

## 2.3. Naive Bayes Introduction

Äá»ƒ xÃ¢y dá»±ng bá»™ phÃ¢n loáº¡i, trÆ°á»›c tiÃªn chÃºng ta sáº½ báº¯t Ä‘áº§u báº±ng cÃ¡ch táº¡o cÃ¡c xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n theo báº£ng sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W230.png" width="500"/>
</p>

Äiá»u nÃ y cho phÃ©p chÃºng ta tÃ­nh toÃ¡n báº£ng xÃ¡c suáº¥t sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W231.png" width="500"/>
</p>

Khi báº¡n Ä‘Ã£ cÃ³ xÃ¡c suáº¥t, báº¡n cÃ³ thá»ƒ tÃ­nh likelihood score nhÆ° sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W232.png" width="500"/>
</p>

Äiá»ƒm lá»›n hÆ¡n 1 biá»ƒu thá»‹ lá»›p Ä‘Ã³ lÃ  tÃ­ch cá»±c, ngÆ°á»£c láº¡i lÃ  tiÃªu cá»±c

## 2.4. Laplacian Smoothing

ChÃºng tÃ´i thÆ°á»ng tÃ­nh xÃ¡c suáº¥t cá»§a má»™t tá»« cho má»™t lá»›p nhÆ° sau:

$$
P(w_i \mid \text{class}) = \frac{\text{freq}(w_i, \text{class})}{N_{\text{class}}} \quad \text{class} \in \{ \text{Positive}, \text{Negative} \}
$$

Tuy nhiÃªn, náº¿u má»™t tá»« khÃ´ng xuáº¥t hiá»‡n trong quÃ¡ trÃ¬nh Ä‘Ã o táº¡o, thÃ¬ nÃ³ sáº½ tá»± Ä‘á»™ng nháº­n Ä‘Æ°á»£c xÃ¡c suáº¥t lÃ  0, Ä‘á»ƒ kháº¯c phá»¥c Ä‘iá»u nÃ y, chÃºng tÃ´i thÃªm smoothing nhÆ° sau

$$
P(w_i \mid \text{class}) = \frac{\text{freq}(w_i, \text{class}) + 1}{N_{\text{class}} + V}
$$

LÆ°u Ã½ ráº±ng chÃºng ta Ä‘Ã£ thÃªm $1$ vÃ o tá»­ sá»‘ vÃ  vÃ¬ cÃ³ $V$ tá»« cáº§n chuáº©n hÃ³a, nÃªn chÃºng ta thÃªm $V$ vÃ² máº«u sá»‘.

$ğ‘_{ğ‘ğ‘™ğ‘ğ‘ ğ‘ }$: táº§n suáº¥t cá»§a táº¥t cáº£ cÃ¡c tá»« trong lá»›p

$V$: sá»‘ lÆ°á»£ng tá»« duy nháº¥t trong vocabulary

## 2.5. **Log Likelihood**

Äá»ƒ tÃ­nh log likelihood, chÃºng ta cáº§n láº¥y cÃ¡c tá»· lá»‡ vÃ  sá»­ dá»¥ng chÃºng Ä‘á»ƒ tÃ­nh score cho phÃ©p chÃºng ta quyáº¿t Ä‘á»‹nh xem má»™t tweet lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c. Tá»· lá»‡ cÃ ng cao, thÃ¬ tá»« Ä‘Ã³ cÃ ng tÃ­ch cá»±c:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W250.png" width="500"/>
</p>

Theo cÃ´ng thá»©c toÃ¡n thÃ¬ biá»ƒu diá»…n nhÆ° sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W251.png" width="300"/>
</p>

Khi $*m$* lá»›n hÆ¡n, chÃºng ta cÃ³ thá»ƒ gáº·p pháº£i váº¥n Ä‘á» vá» viá»‡c sá»‘ sáº½ quÃ¡ nhá», do Ä‘Ã³ chÃºng ta sáº½ sá»­ dá»¥ng $\log$, ta cÃ³ phÆ°Æ¡ng trÃ¬nh nhÆ° sau: 

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W252.png" width="500"/>
</p>

ThÃ nh pháº§n Ä‘áº§u tiÃªn Ä‘Æ°á»£c gá»i lÃ  log prior vÃ  thÃ nh pháº§n thá»© hai lÃ  log likelihood. ChÃºng tÃ´i giá»›i thiá»‡u thÃªm $ğœ†$ nhÆ° sau:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W253.png" width="500"/>
</p>

Sau khi chÃºng ta tÃ­nh toÃ¡n Ä‘Æ°á»£c $ğœ†$ dictionary, viá»‡c suy luáº­n trá»Ÿ nÃªn Ä‘Æ¡n giáº£n:

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W254.png" width="500"/>
</p>

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y á»Ÿ trÃªn, vÃ¬ $3.3>0$, chÃºng ta sáº½ phÃ¢n loáº¡i tÃ i liá»‡u lÃ  dÆ°Æ¡ng. Náº¿u chÃºng ta nháº­n Ä‘Æ°á»£c sá»‘ Ã¢m, chÃºng ta sáº½ phÃ¢n loáº¡i nÃ³ vÃ o lá»›p Ã¢m.

## 2.6. Training naÃ¯ve Bayes

Äá»ƒ train mÃ´ hÃ¬nh naÃ¯ve Bayes classifier, chÃºng ta pháº£i thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau:

**1) Chuáº©n bá»‹ má»™t bá»™ dataset tweets cÃ³ 2 lá»›p positive vÃ  negative.**

**2) Preprocess the tweets: process_tweet(tweet) â [w1, w2, w3, ...]:**

- Lowercase - Chuyá»ƒn sang chá»¯ thÆ°á»ng
- Remove punctuation, urls, names - XÃ³a dáº¥u cÃ¢u, URLs vÃ  tÃªn riÃªng
- Remove stop words - XÃ³a stop words
- Stemming - Chuáº©n hÃ³a tá»«
- Tokenize sentences - TÃ¡ch tá»«

**3) TÃ­nh freq(w, class):**

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W260.png" width="500"/>
</p>

**4) Get $ğ‘ƒ(ğ‘¤âˆ£ğ‘ğ‘œğ‘ ),ğ‘ƒ(ğ‘¤âˆ£ğ‘›ğ‘’ğ‘”)$**

ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng báº£ng trÃªn Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t.

**5) TÃ­nh $ğœ†(ğ‘¤)$**

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W261.png" width="300"/>
</p>

**6) TÃ­nh $logprior=\log(P(pos)/P(neg))$**

$logprior=\log\displaystyle\frac{D_{pos}}{D_{neg}}$ Trong Ä‘Ã³, $D_{pos}$ vÃ  $D_{neg}$ tÆ°Æ¡ng á»©ng vá»›i sá»‘ lÆ°á»£ng sample positive vÃ  negative.

## 2.7. Testing naÃ¯ve Bayes

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W270.png" width="500"/>
</p>

VÃ­ dá»¥ trÃªn cho tháº¥y cÃ¡ch báº¡n cÃ³ thá»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n dá»±a trÃªn $*Î»$* dictionary. Trong vÃ­ dá»¥ nÃ y, $ğ‘™ğ‘œğ‘”ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ$ lÃ  0 chÃºng ta cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng tweets cá»§a positive vÃ  negative ($\log1=0$).

## 2.8. Applications of NaÃ¯ve Bayes

CÃ³ nhiá»u á»©ng dá»¥ng cá»§a naÃ¯ve Bayes bao gá»“m:

- Author Identification - XÃ¡c Ä‘á»‹nh tÃ¡c giáº£
- Spam Filtering - Lá»c thÆ° rÃ¡c
- Information Retrieval - Truy xuáº¥t thÃ´ng tin
- Word Disambiguation - PhÃ¢n biá»‡t tá»« ngá»¯

PhÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ¡n giáº£n. NÃ³ cÅ©ng thá»±c sá»± nhanh.

## 2.9. NaÃ¯ve Bayes Assumptions

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W290.png" width="600"/>
</p>

Giáº£ Ä‘á»‹nh chÃ­nh cá»§a NaÃ¯ve Bayes lÃ  cÃ¡c tá»« trong má»™t cÃ¢u Ä‘á»™c láº­p vá»›i nhau. Tuy nhiÃªn, giáº£ Ä‘á»‹nh nÃ y cÃ³ thá»ƒ lÃ  má»™t váº¥n Ä‘á» vÃ¬ cÃ¡c tá»« trong má»™t cÃ¢u thÆ°á»ng cÃ³ liÃªn quan vá»›i nhau.

- VÃ­ dá»¥ nhÆ° náº¿u chÃºng ta cÃ³ cÃ¢u "Trá»i náº¯ng vÃ  nÃ³ng á»Ÿ sa máº¡c Sahara", cÃ¡c tá»« "náº¯ng" vÃ  "nÃ³ng" thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng nhau vÃ  cÃ³ liÃªn quan vá»›i nhau. NhÆ°ng NaÃ¯ve Bayes cho ráº±ng cÃ¡c tá»« trong má»™t cÃ¢u lÃ  Ä‘á»™c láº­p, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n viá»‡c Ä‘Ã¡nh giÃ¡ tháº¥p hoáº·c Ä‘Ã¡nh giÃ¡ cao xÃ¡c suáº¥t cá»§a tá»«ng tá»«.

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W291.png" width="600"/>
</p>

Váº¥n Ä‘á» khÃ¡c cá»§a NaÃ¯ve Bayes lÃ  nÃ³ phá»¥ thuá»™c vÃ o phÃ¢n phá»‘i cá»§a táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. Náº¿u táº­p dá»¯ liá»‡u huáº¥n luyá»‡n khÃ´ng Ä‘áº¡i diá»‡n cho dá»¯ liá»‡u thá»±c táº¿, hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng.

- VÃ­ dá»¥, náº¿u cÃ¡c tweet tÃ­ch cá»±c xuáº¥t hiá»‡n nhiá»u hÆ¡n so vá»›i cÃ¡c tweet tiÃªu cá»±c trong dá»¯ liá»‡u thá»±c táº¿, nhÆ°ng táº­p dá»¯ liá»‡u huáº¥n luyá»‡n láº¡i cÃ¢n báº±ng, mÃ´ hÃ¬nh cÃ³ thá»ƒ khÃ´ng dá»± Ä‘oÃ¡n Ä‘Ãºng cáº£m xÃºc má»™t cÃ¡ch chÃ­nh xÃ¡c.

Máº·c dÃ¹ cÃ³ nhá»¯ng háº¡n cháº¿ nÃ y, NaÃ¯ve Bayes váº«n cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t trong má»™t sá»‘ tÃ¬nh huá»‘ng

## 2.10. Error Analysis

Khi chÃºng ta sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p NLP Ä‘á»ƒ phÃ¢n tÃ­ch vÄƒn báº£n, cÃ³ thá»ƒ xáº£y ra cÃ¡c lá»—i. Nhá»¯ng lá»—i nÃ y cÃ³ thá»ƒ xáº£y ra vÃ¬ nhiá»u lÃ½ do khÃ¡c nhau.

- Lost Semantic Meaning: ÄÃ´i khi, trong quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½, Ã½ nghÄ©a cá»§a má»™t cÃ¢u cÃ³ thá»ƒ bá»‹ máº¥t. VÃ­ dá»¥, náº¿u chÃºng ta loáº¡i bá» dáº¥u cháº¥m cÃ¢u tá»« má»™t tweet biá»ƒu thá»‹ má»™t khuÃ´n máº·t buá»“n, tweet Ä‘Ã£ qua xá»­ lÃ½ cÃ³ thá»ƒ chá»‰ chá»©a nhá»¯ng tá»« tÃ­ch cá»±c, táº¡o ra má»™t cáº£m xÃºc khÃ¡c.

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W2100.png" width="500"/>
</p>

- Word Order: Thá»© tá»± cá»§a cÃ¡c tá»« trong má»™t cÃ¢u cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n Ã½ nghÄ©a cá»§a nÃ³. Náº¿u chÃºng ta loáº¡i bá» má»™t sá»‘ tá»« nhÆ° "no" hoáº·c "this," vÄƒn báº£n Ä‘Ã£ qua xá»­ lÃ½ cÃ³ thá»ƒ truyá»n Ä‘áº¡t má»™t cáº£m xÃºc khÃ¡c so vá»›i cÃ¢u gá»‘c.

<p align="center">
  <img src="https://github.com/nvsthinh/NLP_Coursera/blob/main/Course_1/data/W2101.png" width="500"/>
</p>

- Quirks of Language: VÃ­ dá»¥, má»™t bÃ i Ä‘Ã¡nh giÃ¡ phim tÃ­ch cá»±c cÃ³ thá»ƒ chá»©a chá»§ yáº¿u cÃ¡c tá»« tiÃªu cá»±c, dáº«n Ä‘áº¿n má»™t dá»± Ä‘oÃ¡n cáº£m xÃºc sai.